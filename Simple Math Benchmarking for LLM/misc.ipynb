{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohmirzabr/SIMP/blob/main/Simple%20Math%20Benchmarking%20for%20LLM/misc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XhX0ekpWW5JC"
      },
      "outputs": [],
      "source": [
        "# üé® Math LLM Study Assistant Experiment Notebook\n",
        "#@title üë®‚Äçüî¨ Simple Mathematical Benchmarking for Natural Language Processing Models\n",
        "#@markdown This notebook version is the preseved version of my recent research outputs. Based on the [main notebook](https://github.com/mohmirzabr/SIMP/blob/main/1.%20Simple%20Mathematical%20Benchmarking%20for%20Natural%20Language%20Processing%20Models/MAIN.ipynb) from the repository which is developed together with ChatGPT by OpenAI that I attributed to. This notebook guides you step-by-step to compare atleast three natural language processing models from Ollama on basic math questions from GSK8M (openai/gsk8m).\n",
        "#@markdown <br> <br> This project is so simple that it only could benchmark simple mathemathics like GSM8K. As the result comparing only compare the numbers from the generation result and looking for the supposedly correct number, whether it is mentioned in the generation result or not.\n",
        "#@markdown <br> <br> In conclusion, this notebook isn't suitable for advanced benchmarking and only can process simple mathematical prompts.\n",
        "#@markdown <br> <br> Visit me on [GitHub](https://github.com/mohmirzabr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Q1loa5PDhGcp"
      },
      "outputs": [],
      "source": [
        "#@title 0. ‚ö†Ô∏è MAKE SURE YOU'RE USING NVIDIA T4 AS THE GPU AND PYTHON 3 FOR THE RUNTIME TYPE\n",
        "#@markdown This cell will make sure whether you're having your NVIDIA T4 enabled or not.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ScW7aixX4m_"
      },
      "outputs": [],
      "source": [
        "#@title 1. üß∞ Install Ollama\n",
        "# @markdown In this cell, we will install the backend first. In which, we'll be using Ollama.\n",
        "\n",
        "# üöÄ Installing Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rO_2YF4va57s"
      },
      "outputs": [],
      "source": [
        "#@title 2. üìÇ Load Math Questions\n",
        "#@markdown This cell is where we're going to put the samples to be tested to each model. This cell will create a file named 'math_questions.txt' with lines: question<TAB>answer (except if the file existed in the first place).\n",
        "templatequestions = False\n",
        "\n",
        "#@markdown <br> <br> Below are the variables that are going to be used for scraping the dataset from Hugging Face to 'math_questions.txt'.\n",
        "#@markdown <br> <br> 1. üß∞ Which dataset you're going to refer to?\n",
        "dataset = \"openai/gsm8k\" # @param {\"type\":\"string\"}\n",
        "branch = \"main\" # @param {\"type\":\"string\"}\n",
        "#@markdown <br> <br> 2. üìä How many examples to use?\n",
        "NUM_SAMPLES = 100 # @param {\"type\":\"number\"}\n",
        "\n",
        "if templatequestions == False:\n",
        "  from pathlib import Path\n",
        "\n",
        "  questions_file = Path('math_questions.txt')\n",
        "\n",
        "  if not questions_file.exists():\n",
        "      # üîß 1. Install and import the HF datasets library\n",
        "      !pip install -U datasets\n",
        "      from datasets import load_dataset\n",
        "\n",
        "      # üîç 2. Load the GSM8K training split\n",
        "      gsm8k = load_dataset(dataset, branch, split=\"train\")\n",
        "\n",
        "      # ‚úÇÔ∏è 3. Extract question + final answer (after \"####\")\n",
        "      samples = []\n",
        "      for ex in gsm8k.select(range(NUM_SAMPLES)):\n",
        "          q_text = ex[\"question\"].strip()\n",
        "          # the answer field has steps + \"#### <final>\"\n",
        "          raw_ans = ex[\"answer\"]\n",
        "          # split on \"####\" to get the last line as our truth\n",
        "          truth = raw_ans.split(\"####\")[-1].strip()\n",
        "          samples.append((q_text, truth))\n",
        "\n",
        "      # üìù 4. Save those to math_questions.txt as \"question‚êâanswer\"\n",
        "      with open(questions_file, 'w') as f:\n",
        "          for q, a in samples:\n",
        "              f.write(f\"{q}\\t{a}\\n\")\n",
        "\n",
        "      print(f\"‚úÖ Pulled {NUM_SAMPLES} samples from {dataset} and wrote '{questions_file}'\")\n",
        "\n",
        "  # üìñ 5. Now read back the file into `questions` for your experiment loop\n",
        "  questions = []\n",
        "  with open(questions_file) as f:\n",
        "      for line in f:\n",
        "          q, a = line.strip().split(\"\\t\")\n",
        "          questions.append((q, a))\n",
        "\n",
        "  print(f\"‚úÖ Loaded {len(questions)} questions for testing!\")\n",
        "elif templatequestions == True:\n",
        "  from pathlib import Path\n",
        "\n",
        "  questions_file = Path('math_questions.txt')\n",
        "  if not questions_file.exists():\n",
        "      # Create a sample file with simple math problems\n",
        "      sample = [\n",
        "          \"12 x 7\\t84\",\n",
        "          \"17 * 24\\t408\",\n",
        "          \"256 / 8\\t32\",\n",
        "          \"13 + 49\\t62\"\n",
        "      ]\n",
        "      with open(questions_file, 'w') as f:\n",
        "          for line in sample:\n",
        "              f.write(line + '\\n')  # üìù writing sample questions\n",
        "\n",
        "  # Read the questions into a list\n",
        "  questions = []\n",
        "  with open(questions_file) as f:\n",
        "      for line in f:\n",
        "          q, a = line.strip().split(\"\\t\")\n",
        "          questions.append((q, a))\n",
        "\n",
        "  print(f\"‚úÖ Loaded {len(questions)} questions for testing!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sjrZiBHT5Xkf"
      },
      "outputs": [],
      "source": [
        "#@title 3. üß∞ First Model: tinyllama:1.1b\n",
        "# @markdown In this cell, we'll pull the model to be tested in the next cells.\n",
        "model = \"tinyllama:1.1b\" #@param {\"type\" : \"string\"}\n",
        "\n",
        "# üî• Enabling Ollama\n",
        "import subprocess\n",
        "process = subprocess.Popen (\"ollama serve\", shell=True)\n",
        "\n",
        "# ‚ôªÔ∏è Pulling the models.\n",
        "!ollama pull {model}\n",
        "\n",
        "# üì¶ Confirming the models that are ready.\n",
        "!ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LlQYcfgue04P"
      },
      "outputs": [],
      "source": [
        "#@title üèÅ 4. Set Up Results Logging\n",
        "#@markdown We will record: question, model, answer, correct (True/False), time in seconds and put it into a CSV file to be processed by the next cell and then compared for conclusion.\n",
        "\n",
        "import csv\n",
        "\n",
        "output_file = f\"results_{model.replace(':','_')}.csv\"\n",
        "with open(output_file, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    # Header row for our CSV\n",
        "    writer.writerow(['question', 'model', 'answer', 'correct', 'time_s', 'mem_delta_MiB'])\n",
        "print(f\"üóÑÔ∏è  Results will be saved in: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wb2ARGg3fIAk"
      },
      "outputs": [],
      "source": [
        "#@title 5. üöÄ Experiment is running!\n",
        "#@markdown This is the cell where everything begin to run, and the GPU RAM start to shrinking. The notebook is processing all of the questions and comparing the answer at the same time!\n",
        "\n",
        "import time, subprocess, csv, re\n",
        "\n",
        "selected_model = model  # from your dropdown cell\n",
        "\n",
        "# Helper to read GPU memory\n",
        "def get_gpu_mem():\n",
        "    out = subprocess.run(\n",
        "        ['nvidia-smi',\n",
        "         '--query-gpu=memory.used',\n",
        "         '--format=csv,noheader,nounits'],\n",
        "        stdout=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "    return int(out.stdout.strip())\n",
        "\n",
        "# 1Ô∏è‚É£ Measure GPU memory before loading/running the model\n",
        "mem_before = get_gpu_mem()\n",
        "print(f\"üè∑Ô∏è  Running model: {selected_model}\")\n",
        "print(f\"‚ñ∂Ô∏è  GPU memory BEFORE: {mem_before} MiB\\n\")\n",
        "\n",
        "# 2Ô∏è‚É£ Prepare CSV logging (append mode; header assumed already written)\n",
        "#    If you need to reinitialize, uncomment the lines below:\n",
        "# import csv\n",
        "# with open(output_file, 'w', newline='') as csvfile:\n",
        "#     writer = csv.writer(csvfile)\n",
        "#     writer.writerow(['question', 'model', 'answer', 'correct', 'time_s', 'mem_delta_MiB'])\n",
        "\n",
        "# 3Ô∏è‚É£ Loop through questions\n",
        "for question, truth in questions:\n",
        "    # Print the question\n",
        "    print(f\"\\nüîç [Q] {question}\")\n",
        "\n",
        "    # Run the model and time it\n",
        "    t0 = time.time()\n",
        "    result = subprocess.run(\n",
        "        ['ollama', 'run', selected_model, question],\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
        "    )\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    # Measure memory after (model already loaded)\n",
        "    mem_after = get_gpu_mem()\n",
        "    mem_delta = mem_after - mem_before\n",
        "\n",
        "    # Parse the model‚Äôs output\n",
        "    if result.returncode != 0:\n",
        "        answer = result.stderr.strip()\n",
        "    else:\n",
        "        lines = [l for l in result.stdout.strip().split('\\n') if l.strip()]\n",
        "        answer = lines[-1] if lines else \"\"\n",
        "\n",
        "    # üîç Diagnostic\n",
        "    print(f\"üó®Ô∏è  Answer: {answer}\")\n",
        "    print(f\"üîé Looking for numeric token: '{truth}'\")\n",
        "\n",
        "    # === Numeric matching ===\n",
        "    # Extract all integer/decimal tokens\n",
        "    found_numbers = re.findall(r'\\d+\\.?\\d*', answer)\n",
        "    # Convert them to floats\n",
        "    nums = [float(tok) for tok in found_numbers]\n",
        "    ## Compare to the truth value\n",
        "    #truth_val = float(truth)\n",
        "    # Remove commas from the ground‚Äêtruth (e.g. \"1,080\" ‚Üí \"1080\")\n",
        "    truth_clean = truth.replace(',', '')\n",
        "    truth_val   = float(truth_clean)\n",
        "    correct = any(abs(n - truth_val) < 1e-6 for n in nums)\n",
        "\n",
        "    # Diagnostics\n",
        "    print(f\"üî¢  Extracted numeric tokens: {nums}\")\n",
        "    print(f\"‚úîÔ∏è Correct? {correct}\")\n",
        "\n",
        "    mark = '‚úÖ' if correct else '‚ùå'\n",
        "    print(f\"{mark}  ({elapsed:.2f}s, ŒîRAM={mem_delta}‚ÄØMiB)\")\n",
        "\n",
        "    # 4Ô∏è‚É£ Log to CSV\n",
        "    with open(output_file, 'a', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\n",
        "            question,\n",
        "            selected_model,\n",
        "            answer,\n",
        "            correct,\n",
        "            f\"{elapsed:.2f}\",\n",
        "            mem_delta\n",
        "        ])\n",
        "\n",
        "# 5Ô∏è‚É£ Final memory report\n",
        "print(f\"\\n‚úîÔ∏è  Finished running {selected_model}. Final GPU RAM used: {mem_after}‚ÄØMiB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "t56TBh83gGog"
      },
      "outputs": [],
      "source": [
        "#@title 6. üìä Summarize & Compare Results\n",
        "#@markdown This is where everything begin to be put into a conclusion for a single model. By using pandas, this cell loads the CSV file and compute per-model metrics: Accuracy (%), Average inference time (seconds), and Memory Usage (MiB).\n",
        "\n",
        "# ‚úÖ Summary Code for Single-Model Runs (New Format)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the result file from current model\n",
        "file_path = f'results_{selected_model.replace(\":\", \"_\")}.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert types as needed\n",
        "df['correct_num'] = df['correct'].astype(int)\n",
        "df['time_s'] = df['time_s'].astype(float)\n",
        "df['mem_delta_MiB'] = df['mem_delta_MiB'].astype(float)\n",
        "\n",
        "# Calculate summary metrics\n",
        "accuracy = df['correct_num'].mean() * 100\n",
        "avg_time = df['time_s'].mean()\n",
        "avg_mem = df['mem_delta_MiB'].mean()\n",
        "\n",
        "# Create a summary DataFrame for display\n",
        "summary_df = pd.DataFrame([{\n",
        "    'Model': selected_model,\n",
        "    'Accuracy (%)': round(accuracy, 1),\n",
        "    'Avg Latency (s)': round(avg_time, 2),\n",
        "    'GPU Memory Used (MiB)': round(avg_mem, 1)\n",
        "}])\n",
        "\n",
        "# üßæ Show table\n",
        "from IPython.display import display\n",
        "print(\"üìà Overall Performance Summary:\")\n",
        "display(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gtDsoNGwcuaQ"
      },
      "outputs": [],
      "source": [
        "#@title 7. üßπ Stopping Ollama Server\n",
        "#@markdown In order to keep the memory management measured accurately, let's stop the whole process and load the model again.\n",
        "#@markdown <br> <br> This cell clears the previous Ollama server, loads the selected model, and captures GPU memory usage for benchmarking.\n",
        "\n",
        "import subprocess, time\n",
        "\n",
        "selected_model = model  # ‚Üê assumes you have a dropdown or text input earlier\n",
        "\n",
        "# Kill any existing Ollama server\n",
        "!pkill -f \"ollama serve\" || echo \"No existing Ollama server.\"\n",
        "\n",
        "# Wait briefly just to be safe\n",
        "time.sleep(2)\n",
        "\n",
        "# Pull the model (if not already pulled)\n",
        "print(f\"üì• Pulling model: {selected_model}\")\n",
        "!ollama pull {selected_model}\n",
        "\n",
        "# Measure memory BEFORE loading the model\n",
        "def get_gpu_mem():\n",
        "    result = subprocess.run(\n",
        "        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n",
        "        stdout=subprocess.PIPE, text=True\n",
        "    )\n",
        "    return int(result.stdout.strip())\n",
        "\n",
        "mem_before = get_gpu_mem()\n",
        "print(f\"üìâ GPU memory BEFORE loading model: {mem_before} MiB\")\n",
        "\n",
        "# Start Ollama server\n",
        "print(\"üöÄ Starting Ollama server...\")\n",
        "server_process = subprocess.Popen(f\"ollama serve\", shell=True)\n",
        "\n",
        "# Allow time for the model to load into memory\n",
        "print(\"‚è≥ Waiting for model to load into GPU...\")\n",
        "time.sleep(8)  # increase if model loads slowly\n",
        "\n",
        "# Measure memory AFTER loading the model\n",
        "mem_after = get_gpu_mem()\n",
        "print(f\"üìà GPU memory AFTER loading model: {mem_after} MiB\")\n",
        "\n",
        "# Save memory delta\n",
        "mem_delta = mem_after - mem_before\n",
        "print(f\"üîç Model GPU Memory Footprint: {mem_delta} MiB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aowQENPfYnlW"
      },
      "outputs": [],
      "source": [
        "#@title 8. üß∞ Second Model: tinyllama:1.1b\n",
        "# @markdown In this cell, we'll pull the model to be tested in the next cells.\n",
        "model = \"phi3:3.8b\" #@param {\"type\" : \"string\"}\n",
        "\n",
        "# üî• Enabling Ollama\n",
        "import subprocess\n",
        "process = subprocess.Popen (\"ollama serve\", shell=True)\n",
        "\n",
        "# ‚ôªÔ∏è Pulling the models.\n",
        "!ollama pull {model}\n",
        "\n",
        "# üì¶ Confirming the models that are ready.\n",
        "!ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VSDBWOi1YnlX"
      },
      "outputs": [],
      "source": [
        "#@title üèÅ 9. Set Up Results Logging\n",
        "#@markdown We will record: question, model, answer, correct (True/False), time in seconds and put it into a CSV file to be processed by the next cell and then compared for conclusion.\n",
        "\n",
        "import csv\n",
        "\n",
        "output_file = f\"results_{model.replace(':','_')}.csv\"\n",
        "with open(output_file, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    # Header row for our CSV\n",
        "    writer.writerow(['question', 'model', 'answer', 'correct', 'time_s', 'mem_delta_MiB'])\n",
        "print(f\"üóÑÔ∏è  Results will be saved in: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_yEmchawYnlX"
      },
      "outputs": [],
      "source": [
        "#@title 10. üöÄ Experiment is running!\n",
        "#@markdown This is the cell where everything begin to run, and the GPU RAM start to shrinking. The notebook is processing all of the questions and comparing the answer at the same time!\n",
        "\n",
        "import time, subprocess, csv, re\n",
        "\n",
        "selected_model = model  # from your dropdown cell\n",
        "\n",
        "# Helper to read GPU memory\n",
        "def get_gpu_mem():\n",
        "    out = subprocess.run(\n",
        "        ['nvidia-smi',\n",
        "         '--query-gpu=memory.used',\n",
        "         '--format=csv,noheader,nounits'],\n",
        "        stdout=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "    return int(out.stdout.strip())\n",
        "\n",
        "# 1Ô∏è‚É£ Measure GPU memory before loading/running the model\n",
        "mem_before = get_gpu_mem()\n",
        "print(f\"üè∑Ô∏è  Running model: {selected_model}\")\n",
        "print(f\"‚ñ∂Ô∏è  GPU memory BEFORE: {mem_before} MiB\\n\")\n",
        "\n",
        "# 2Ô∏è‚É£ Prepare CSV logging (append mode; header assumed already written)\n",
        "#    If you need to reinitialize, uncomment the lines below:\n",
        "# import csv\n",
        "# with open(output_file, 'w', newline='') as csvfile:\n",
        "#     writer = csv.writer(csvfile)\n",
        "#     writer.writerow(['question', 'model', 'answer', 'correct', 'time_s', 'mem_delta_MiB'])\n",
        "\n",
        "# 3Ô∏è‚É£ Loop through questions\n",
        "for question, truth in questions:\n",
        "    # Print the question\n",
        "    print(f\"\\nüîç [Q] {question}\")\n",
        "\n",
        "    # Run the model and time it\n",
        "    t0 = time.time()\n",
        "    result = subprocess.run(\n",
        "        ['ollama', 'run', selected_model, question],\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
        "    )\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    # Measure memory after (model already loaded)\n",
        "    mem_after = get_gpu_mem()\n",
        "    mem_delta = mem_after - mem_before\n",
        "\n",
        "    # Parse the model‚Äôs output\n",
        "    if result.returncode != 0:\n",
        "        answer = result.stderr.strip()\n",
        "    else:\n",
        "        lines = [l for l in result.stdout.strip().split('\\n') if l.strip()]\n",
        "        answer = lines[-1] if lines else \"\"\n",
        "\n",
        "    # üîç Diagnostic\n",
        "    print(f\"üó®Ô∏è  Answer: {answer}\")\n",
        "    print(f\"üîé Looking for numeric token: '{truth}'\")\n",
        "\n",
        "    # === Numeric matching ===\n",
        "    # Extract all integer/decimal tokens\n",
        "    found_numbers = re.findall(r'\\d+\\.?\\d*', answer)\n",
        "    # Convert them to floats\n",
        "    nums = [float(tok) for tok in found_numbers]\n",
        "    ## Compare to the truth value\n",
        "    #truth_val = float(truth)\n",
        "    # Remove commas from the ground‚Äêtruth (e.g. \"1,080\" ‚Üí \"1080\")\n",
        "    truth_clean = truth.replace(',', '')\n",
        "    truth_val   = float(truth_clean)\n",
        "    correct = any(abs(n - truth_val) < 1e-6 for n in nums)\n",
        "\n",
        "    # Diagnostics\n",
        "    print(f\"üî¢  Extracted numeric tokens: {nums}\")\n",
        "    print(f\"‚úîÔ∏è Correct? {correct}\")\n",
        "\n",
        "    mark = '‚úÖ' if correct else '‚ùå'\n",
        "    print(f\"{mark}  ({elapsed:.2f}s, ŒîRAM={mem_delta}‚ÄØMiB)\")\n",
        "\n",
        "    # 4Ô∏è‚É£ Log to CSV\n",
        "    with open(output_file, 'a', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\n",
        "            question,\n",
        "            selected_model,\n",
        "            answer,\n",
        "            correct,\n",
        "            f\"{elapsed:.2f}\",\n",
        "            mem_delta\n",
        "        ])\n",
        "\n",
        "# 5Ô∏è‚É£ Final memory report\n",
        "print(f\"\\n‚úîÔ∏è  Finished running {selected_model}. Final GPU RAM used: {mem_after}‚ÄØMiB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "v9YjxuaTYnlX"
      },
      "outputs": [],
      "source": [
        "#@title 11. üìä Summarize & Compare Results\n",
        "#@markdown This is where everything begin to be put into a conclusion for a single model. By using pandas, this cell loads the CSV file and compute per-model metrics: Accuracy (%), Average inference time (seconds), and Memory Usage (MiB).\n",
        "\n",
        "# ‚úÖ Summary Code for Single-Model Runs (New Format)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the result file from current model\n",
        "file_path = f'results_{selected_model.replace(\":\", \"_\")}.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert types as needed\n",
        "df['correct_num'] = df['correct'].astype(int)\n",
        "df['time_s'] = df['time_s'].astype(float)\n",
        "df['mem_delta_MiB'] = df['mem_delta_MiB'].astype(float)\n",
        "\n",
        "# Calculate summary metrics\n",
        "accuracy = df['correct_num'].mean() * 100\n",
        "avg_time = df['time_s'].mean()\n",
        "avg_mem = df['mem_delta_MiB'].mean()\n",
        "\n",
        "# Create a summary DataFrame for display\n",
        "summary_df = pd.DataFrame([{\n",
        "    'Model': selected_model,\n",
        "    'Accuracy (%)': round(accuracy, 1),\n",
        "    'Avg Latency (s)': round(avg_time, 2),\n",
        "    'GPU Memory Used (MiB)': round(avg_mem, 1)\n",
        "}])\n",
        "\n",
        "# üßæ Show table\n",
        "from IPython.display import display\n",
        "print(\"üìà Overall Performance Summary:\")\n",
        "display(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qutziQazfmPe"
      },
      "outputs": [],
      "source": [
        "#@title 12. üßπ Stopping Ollama Server\n",
        "#@markdown In order to keep the memory management measured accurately, let's stop the whole process and load the model again.\n",
        "#@markdown <br> <br> This cell clears the previous Ollama server, loads the selected model, and captures GPU memory usage for benchmarking.\n",
        "\n",
        "import subprocess, time\n",
        "\n",
        "selected_model = model  # ‚Üê assumes you have a dropdown or text input earlier\n",
        "\n",
        "# Kill any existing Ollama server\n",
        "!pkill -f \"ollama serve\" || echo \"No existing Ollama server.\"\n",
        "\n",
        "# Wait briefly just to be safe\n",
        "time.sleep(2)\n",
        "\n",
        "# Pull the model (if not already pulled)\n",
        "print(f\"üì• Pulling model: {selected_model}\")\n",
        "!ollama pull {selected_model}\n",
        "\n",
        "# Measure memory BEFORE loading the model\n",
        "def get_gpu_mem():\n",
        "    result = subprocess.run(\n",
        "        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n",
        "        stdout=subprocess.PIPE, text=True\n",
        "    )\n",
        "    return int(result.stdout.strip())\n",
        "\n",
        "mem_before = get_gpu_mem()\n",
        "print(f\"üìâ GPU memory BEFORE loading model: {mem_before} MiB\")\n",
        "\n",
        "# Start Ollama server\n",
        "print(\"üöÄ Starting Ollama server...\")\n",
        "server_process = subprocess.Popen(f\"ollama serve\", shell=True)\n",
        "\n",
        "# Allow time for the model to load into memory\n",
        "print(\"‚è≥ Waiting for model to load into GPU...\")\n",
        "time.sleep(8)  # increase if model loads slowly\n",
        "\n",
        "# Measure memory AFTER loading the model\n",
        "mem_after = get_gpu_mem()\n",
        "print(f\"üìà GPU memory AFTER loading model: {mem_after} MiB\")\n",
        "\n",
        "# Save memory delta\n",
        "mem_delta = mem_after - mem_before\n",
        "print(f\"üîç Model GPU Memory Footprint: {mem_delta} MiB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ICre56U1Yn95"
      },
      "outputs": [],
      "source": [
        "#@title 13. üß∞ Third Model: tinyllama:1.1b\n",
        "# @markdown In this cell, we'll pull the model to be tested in the next cells.\n",
        "model = \"mistral:7b\" #@param {\"type\" : \"string\"}\n",
        "\n",
        "# üî• Enabling Ollama\n",
        "import subprocess\n",
        "process = subprocess.Popen (\"ollama serve\", shell=True)\n",
        "\n",
        "# ‚ôªÔ∏è Pulling the models.\n",
        "!ollama pull {model}\n",
        "\n",
        "# üì¶ Confirming the models that are ready.\n",
        "!ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fLxqBPleYn95"
      },
      "outputs": [],
      "source": [
        "#@title üèÅ 14. Set Up Results Logging\n",
        "#@markdown We will record: question, model, answer, correct (True/False), time in seconds and put it into a CSV file to be processed by the next cell and then compared for conclusion.\n",
        "\n",
        "import csv\n",
        "\n",
        "output_file = f\"results_{model.replace(':','_')}.csv\"\n",
        "with open(output_file, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    # Header row for our CSV\n",
        "    writer.writerow(['question', 'model', 'answer', 'correct', 'time_s', 'mem_delta_MiB'])\n",
        "print(f\"üóÑÔ∏è  Results will be saved in: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "O_GltXRaYn95"
      },
      "outputs": [],
      "source": [
        "#@title 15. üöÄ Experiment is running!\n",
        "#@markdown This is the cell where everything begin to run, and the GPU RAM start to shrinking. The notebook is processing all of the questions and comparing the answer at the same time!\n",
        "\n",
        "import time, subprocess, csv, re\n",
        "\n",
        "selected_model = model  # from your dropdown cell\n",
        "\n",
        "# Helper to read GPU memory\n",
        "def get_gpu_mem():\n",
        "    out = subprocess.run(\n",
        "        ['nvidia-smi',\n",
        "         '--query-gpu=memory.used',\n",
        "         '--format=csv,noheader,nounits'],\n",
        "        stdout=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "    return int(out.stdout.strip())\n",
        "\n",
        "# 1Ô∏è‚É£ Measure GPU memory before loading/running the model\n",
        "mem_before = get_gpu_mem()\n",
        "print(f\"üè∑Ô∏è  Running model: {selected_model}\")\n",
        "print(f\"‚ñ∂Ô∏è  GPU memory BEFORE: {mem_before} MiB\\n\")\n",
        "\n",
        "# 2Ô∏è‚É£ Prepare CSV logging (append mode; header assumed already written)\n",
        "#    If you need to reinitialize, uncomment the lines below:\n",
        "# import csv\n",
        "# with open(output_file, 'w', newline='') as csvfile:\n",
        "#     writer = csv.writer(csvfile)\n",
        "#     writer.writerow(['question', 'model', 'answer', 'correct', 'time_s', 'mem_delta_MiB'])\n",
        "\n",
        "# 3Ô∏è‚É£ Loop through questions\n",
        "for question, truth in questions:\n",
        "    # Print the question\n",
        "    print(f\"\\nüîç [Q] {question}\")\n",
        "\n",
        "    # Run the model and time it\n",
        "    t0 = time.time()\n",
        "    result = subprocess.run(\n",
        "        ['ollama', 'run', selected_model, question],\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
        "    )\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    # Measure memory after (model already loaded)\n",
        "    mem_after = get_gpu_mem()\n",
        "    mem_delta = mem_after - mem_before\n",
        "\n",
        "    # Parse the model‚Äôs output\n",
        "    if result.returncode != 0:\n",
        "        answer = result.stderr.strip()\n",
        "    else:\n",
        "        lines = [l for l in result.stdout.strip().split('\\n') if l.strip()]\n",
        "        answer = lines[-1] if lines else \"\"\n",
        "\n",
        "    # üîç Diagnostic\n",
        "    print(f\"üó®Ô∏è  Answer: {answer}\")\n",
        "    print(f\"üîé Looking for numeric token: '{truth}'\")\n",
        "\n",
        "    # === Numeric matching ===\n",
        "    # Extract all integer/decimal tokens\n",
        "    found_numbers = re.findall(r'\\d+\\.?\\d*', answer)\n",
        "    # Convert them to floats\n",
        "    nums = [float(tok) for tok in found_numbers]\n",
        "    ## Compare to the truth value\n",
        "    #truth_val = float(truth)\n",
        "    # Remove commas from the ground‚Äêtruth (e.g. \"1,080\" ‚Üí \"1080\")\n",
        "    truth_clean = truth.replace(',', '')\n",
        "    truth_val   = float(truth_clean)\n",
        "    correct = any(abs(n - truth_val) < 1e-6 for n in nums)\n",
        "\n",
        "    # Diagnostics\n",
        "    print(f\"üî¢  Extracted numeric tokens: {nums}\")\n",
        "    print(f\"‚úîÔ∏è Correct? {correct}\")\n",
        "\n",
        "    mark = '‚úÖ' if correct else '‚ùå'\n",
        "    print(f\"{mark}  ({elapsed:.2f}s, ŒîRAM={mem_delta}‚ÄØMiB)\")\n",
        "\n",
        "    # 4Ô∏è‚É£ Log to CSV\n",
        "    with open(output_file, 'a', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\n",
        "            question,\n",
        "            selected_model,\n",
        "            answer,\n",
        "            correct,\n",
        "            f\"{elapsed:.2f}\",\n",
        "            mem_delta\n",
        "        ])\n",
        "\n",
        "# 5Ô∏è‚É£ Final memory report\n",
        "print(f\"\\n‚úîÔ∏è  Finished running {selected_model}. Final GPU RAM used: {mem_after}‚ÄØMiB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G-nm6kl8Yn96"
      },
      "outputs": [],
      "source": [
        "#@title 16. üìä Summarize & Compare Results\n",
        "#@markdown This is where everything begin to be put into a conclusion for a single model. By using pandas, this cell loads the CSV file and compute per-model metrics: Accuracy (%), Average inference time (seconds), and Memory Usage (MiB).\n",
        "\n",
        "# ‚úÖ Summary Code for Single-Model Runs (New Format)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the result file from current model\n",
        "file_path = f'results_{selected_model.replace(\":\", \"_\")}.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert types as needed\n",
        "df['correct_num'] = df['correct'].astype(int)\n",
        "df['time_s'] = df['time_s'].astype(float)\n",
        "df['mem_delta_MiB'] = df['mem_delta_MiB'].astype(float)\n",
        "\n",
        "# Calculate summary metrics\n",
        "accuracy = df['correct_num'].mean() * 100\n",
        "avg_time = df['time_s'].mean()\n",
        "avg_mem = df['mem_delta_MiB'].mean()\n",
        "\n",
        "# Create a summary DataFrame for display\n",
        "summary_df = pd.DataFrame([{\n",
        "    'Model': selected_model,\n",
        "    'Accuracy (%)': round(accuracy, 1),\n",
        "    'Avg Latency (s)': round(avg_time, 2),\n",
        "    'GPU Memory Used (MiB)': round(avg_mem, 1)\n",
        "}])\n",
        "\n",
        "# üßæ Show table\n",
        "from IPython.display import display\n",
        "print(\"üìà Overall Performance Summary:\")\n",
        "display(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kziwRlTFfnCu"
      },
      "outputs": [],
      "source": [
        "#@title 17. üßπ Stopping Ollama Server\n",
        "#@markdown In order to keep the memory management measured accurately, let's stop the whole process and load the model again.\n",
        "#@markdown <br> <br> This cell clears the previous Ollama server, loads the selected model, and captures GPU memory usage for benchmarking.\n",
        "\n",
        "import subprocess, time\n",
        "\n",
        "selected_model = model  # ‚Üê assumes you have a dropdown or text input earlier\n",
        "\n",
        "# Kill any existing Ollama server\n",
        "!pkill -f \"ollama serve\" || echo \"No existing Ollama server.\"\n",
        "\n",
        "# Wait briefly just to be safe\n",
        "time.sleep(2)\n",
        "\n",
        "# Pull the model (if not already pulled)\n",
        "print(f\"üì• Pulling model: {selected_model}\")\n",
        "!ollama pull {selected_model}\n",
        "\n",
        "# Measure memory BEFORE loading the model\n",
        "def get_gpu_mem():\n",
        "    result = subprocess.run(\n",
        "        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n",
        "        stdout=subprocess.PIPE, text=True\n",
        "    )\n",
        "    return int(result.stdout.strip())\n",
        "\n",
        "mem_before = get_gpu_mem()\n",
        "print(f\"üìâ GPU memory BEFORE loading model: {mem_before} MiB\")\n",
        "\n",
        "# Start Ollama server\n",
        "print(\"üöÄ Starting Ollama server...\")\n",
        "server_process = subprocess.Popen(f\"ollama serve\", shell=True)\n",
        "\n",
        "# Allow time for the model to load into memory\n",
        "print(\"‚è≥ Waiting for model to load into GPU...\")\n",
        "time.sleep(8)  # increase if model loads slowly\n",
        "\n",
        "# Measure memory AFTER loading the model\n",
        "mem_after = get_gpu_mem()\n",
        "print(f\"üìà GPU memory AFTER loading model: {mem_after} MiB\")\n",
        "\n",
        "# Save memory delta\n",
        "mem_delta = mem_after - mem_before\n",
        "print(f\"üîç Model GPU Memory Footprint: {mem_delta} MiB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2BqdjkzF5Biq"
      },
      "outputs": [],
      "source": [
        "#@title 18. üîÄ Merge All Results & Final Comparison\n",
        "#@markdown This cell will combine all CSV file named after 'result_'. Simply, it will combine all result into one single file named 'results_combined_summary.csv'.\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# 1Ô∏è‚É£ Find all result CSVs in this directory\n",
        "pattern = \"results_*.csv\"\n",
        "files = glob.glob(pattern)\n",
        "if not files:\n",
        "    raise FileNotFoundError(f\"No files matching {pattern} found!\")\n",
        "\n",
        "# 2Ô∏è‚É£ Load each file and extract the model name from its filename\n",
        "all_dfs = []\n",
        "for fn in files:\n",
        "    # e.g. fn = \"results_phi3_3.8b.csv\"\n",
        "    model_name = os.path.basename(fn).removeprefix(\"results_\").removesuffix(\".csv\")\n",
        "    df = pd.read_csv(fn)\n",
        "    df['model'] = model_name\n",
        "    all_dfs.append(df)\n",
        "\n",
        "# 3Ô∏è‚É£ Concatenate into one DataFrame\n",
        "combined = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "# 4Ô∏è‚É£ Compute per-model metrics\n",
        "combined['correct_num'] = combined['correct'].astype(int)\n",
        "combined['time_s']      = combined['time_s'].astype(float)\n",
        "combined['mem_delta_MiB'] = combined['mem_delta_MiB'].astype(float)\n",
        "\n",
        "summary = combined.groupby('model').agg(\n",
        "    Accuracy       = ('correct_num',     lambda x: x.mean() * 100),\n",
        "    Avg_Latency_s  = ('time_s',          'mean'),\n",
        "    Avg_Mem_Use_MiB= ('mem_delta_MiB',   'mean')\n",
        ").round(2).reset_index()\n",
        "\n",
        "# 5Ô∏è‚É£ Display the final comparison table\n",
        "from IPython.display import display\n",
        "print(\"üèÅ Final Comparison of All Models:\")\n",
        "display(summary)\n",
        "\n",
        "# üéâ Optional: save this summary if you like\n",
        "summary.to_csv(\"results_combined_summary.csv\", index=False)\n",
        "print(\"Saved combined summary to results_combined_summary.csv\")\n",
        "\n",
        "# üéâ End of experiment. Review the printed logs and summary to draw conclusions!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}