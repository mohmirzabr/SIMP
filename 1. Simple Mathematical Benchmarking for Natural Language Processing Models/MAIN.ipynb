{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohmirzabr/SIMP/blob/main/1.%20Simple%20Mathematical%20Benchmarking%20for%20Natural%20Language%20Processing%20Models/MAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title If something went wrong with ollama. Turn this cell on!\n",
        "import subprocess\n",
        "process = subprocess.Popen (\"ollama serve\", shell=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kFjH0PV9y5Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhX0ekpWW5JC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# üé® Math LLM Study Assistant Experiment Notebook\n",
        "#@title üë®‚Äçüî¨ Simple Mathematical Benchmarking for Natural Language Processing Models\n",
        "#@markdown This notebook is intended for research purposes only. Based on a legacy notebook from the repository which is developed together with ChatGPT by OpenAI that I attributed to. This notebook guides you step-by-step to compare atleast three natural language processing models from Ollama on basic math questions from GSK8M (openai/gsk8m).\n",
        "#@markdown <br> <br> This project is so simple that it only could benchmark simple mathemathics like GSM8K. As the result comparing only compare the numbers from the generation result and looking for the supposedly correct number, whether it is mentioned in the generation result or not.\n",
        "#@markdown <br> <br> In conclusion, this notebook isn't suitable for advanced benchmarking and only can process simple mathematical prompts.\n",
        "#@markdown <br> <br> Visit me on [GitHub](https://github.com/mohmirzabr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 0. ‚ö†Ô∏è MAKE SURE YOU'RE USING NVIDIA T4 AS THE GPU AND PYTHON 3 FOR THE RUNTIME TYPE\n",
        "#@markdown This cell will make sure whether you're having your NVIDIA T4 enabled or not.\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Q1loa5PDhGcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. üß∞ Install Ollama\n",
        "# @markdown In this cell, we will install the backend first. In which, we'll be using Ollama.\n",
        "\n",
        "# üöÄ Installing Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "_ScW7aixX4m_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. üß∞ Prepare the Model\n",
        "# @markdown In this cell, we'll also pull the model to be tested in the next cells.\n",
        "# @markdown  <br> <br> Below are the variables of each model that you want to use. Make sure that the models are existent in Ollama to be downloaded.\n",
        "model = \"tinyllama:1.1b\" # @param [\"tinyllama:1.1b\",\"phi3:3.8b\",\"mistral:7b\"] {\"allow-input\":true}\n",
        "\n",
        "# üî• Enabling Ollama\n",
        "import subprocess\n",
        "process = subprocess.Popen (\"ollama serve\", shell=True)\n",
        "\n",
        "# ‚ôªÔ∏è Pulling the models.\n",
        "!ollama pull {model}\n",
        "\n",
        "# üì¶ Confirming the models that are ready.\n",
        "!ollama list"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sjrZiBHT5Xkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. üìÇ Load Math Questions\n",
        "#@markdown This cell is where we're going to put the samples to be tested to each model. This cell will create a file named 'math_questions.txt' with lines: question<TAB>answer (except if the file existed in the first place).\n",
        "#@markdown <br> <br> If you haven't prepare any datasets, you can use the template below.\n",
        "templatequestions = False # @param {\"type\":\"boolean\"}\n",
        "\n",
        "#@markdown <br> <br> Below are the variables that are going to be used for scraping the dataset from Hugging Face to 'math_questions.txt'.\n",
        "#@markdown <br> <br> 1. üß∞ Which dataset you're going to refer to?\n",
        "dataset = \"openai/gsm8k\" # @param {\"type\":\"string\"}\n",
        "branch = \"main\" # @param {\"type\":\"string\"}\n",
        "#@markdown <br> <br> 2. üìä How many examples to use?\n",
        "NUM_SAMPLES = 30 # @param {\"type\":\"number\"}\n",
        "\n",
        "if templatequestions == False:\n",
        "  from pathlib import Path\n",
        "\n",
        "  questions_file = Path('math_questions.txt')\n",
        "\n",
        "  if not questions_file.exists():\n",
        "      # üîß 1. Install and import the HF datasets library\n",
        "      !pip install -U datasets\n",
        "      from datasets import load_dataset\n",
        "\n",
        "      # üîç 2. Load the GSM8K training split\n",
        "      gsm8k = load_dataset(dataset, branch, split=\"train\")\n",
        "\n",
        "      # ‚úÇÔ∏è 3. Extract question + final answer (after \"####\")\n",
        "      samples = []\n",
        "      for ex in gsm8k.select(range(NUM_SAMPLES)):\n",
        "          q_text = ex[\"question\"].strip()\n",
        "          # the answer field has steps + \"#### <final>\"\n",
        "          raw_ans = ex[\"answer\"]\n",
        "          # split on \"####\" to get the last line as our truth\n",
        "          truth = raw_ans.split(\"####\")[-1].strip()\n",
        "          samples.append((q_text, truth))\n",
        "\n",
        "      # üìù 4. Save those to math_questions.txt as \"question‚êâanswer\"\n",
        "      with open(questions_file, 'w') as f:\n",
        "          for q, a in samples:\n",
        "              f.write(f\"{q}\\t{a}\\n\")\n",
        "\n",
        "      print(f\"‚úÖ Pulled {NUM_SAMPLES} samples from {dataset} and wrote '{questions_file}'\")\n",
        "\n",
        "  # üìñ 5. Now read back the file into `questions` for your experiment loop\n",
        "  questions = []\n",
        "  with open(questions_file) as f:\n",
        "      for line in f:\n",
        "          q, a = line.strip().split(\"\\t\")\n",
        "          questions.append((q, a))\n",
        "\n",
        "  print(f\"‚úÖ Loaded {len(questions)} questions for testing!\")\n",
        "elif templatequestions == True:\n",
        "  from pathlib import Path\n",
        "\n",
        "  questions_file = Path('math_questions.txt')\n",
        "  if not questions_file.exists():\n",
        "      # Create a sample file with simple math problems\n",
        "      sample = [\n",
        "          \"12 x 7\\t84\",\n",
        "          \"17 * 24\\t408\",\n",
        "          \"256 / 8\\t32\",\n",
        "          \"13 + 49\\t62\"\n",
        "      ]\n",
        "      with open(questions_file, 'w') as f:\n",
        "          for line in sample:\n",
        "              f.write(line + '\\n')  # üìù writing sample questions\n",
        "\n",
        "  # Read the questions into a list\n",
        "  questions = []\n",
        "  with open(questions_file) as f:\n",
        "      for line in f:\n",
        "          q, a = line.strip().split(\"\\t\")\n",
        "          questions.append((q, a))\n",
        "\n",
        "  print(f\"‚úÖ Loaded {len(questions)} questions for testing!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rO_2YF4va57s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üèÅ 3. Set Up Results Logging\n",
        "#@markdown We will record: question, model, answer, correct (True/False), time in seconds and put it into a CSV file to be processed by the next cell and then compared for conclusion.\n",
        "\n",
        "import csv\n",
        "\n",
        "output_file = f\"results_{model.replace(':','_')}.csv\"\n",
        "with open(output_file, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    # Header row for our CSV\n",
        "    writer.writerow(['question', 'model', 'answer', 'correct', 'time_s', 'mem_delta_MiB'])\n",
        "print(f\"üóÑÔ∏è  Results will be saved in: {output_file}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LlQYcfgue04P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. üöÄ Experiment is running!\n",
        "#@markdown This is the cell where everything begin to run, and the GPU RAM start to shrinking. The notebook is processing all of the questions and comparing the answer at the same time!\n",
        "\n",
        "import time, subprocess, csv, re\n",
        "\n",
        "selected_model = model  # your dropdown variable\n",
        "\n",
        "# Helper to read GPU memory (unchanged)\n",
        "def get_gpu_mem():\n",
        "    out = subprocess.run(\n",
        "        ['nvidia-smi',\n",
        "         '--query-gpu=memory.used',\n",
        "         '--format=csv,noheader,nounits'],\n",
        "        stdout=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "    return int(out.stdout.strip())\n",
        "\n",
        "# Sample memory before loading/running the model\n",
        "mem_before = get_gpu_mem()\n",
        "print(f\"üè∑Ô∏è  Running model: {selected_model}\")\n",
        "print(f\"‚ñ∂Ô∏è  GPU memory BEFORE: {mem_before} MiB\\n\")\n",
        "\n",
        "# Run through all questions\n",
        "for question, truth in questions:\n",
        "    # 1Ô∏è‚É£ Print the question\n",
        "    print(f\"\\nüîç [Q] {question}\")\n",
        "\n",
        "    # 2Ô∏è‚É£ Run inference\n",
        "    t0 = time.time()\n",
        "    result = subprocess.run(\n",
        "        ['ollama', 'run', selected_model, question],\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
        "    )\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    # 3Ô∏è‚É£ Memory after (model is already loaded)\n",
        "    mem_after = get_gpu_mem()\n",
        "    mem_delta = mem_after - mem_before\n",
        "\n",
        "    # 4Ô∏è‚É£ Parse model output\n",
        "    if result.returncode != 0:\n",
        "        answer = result.stderr.strip()\n",
        "    else:\n",
        "        lines = [l for l in result.stdout.strip().split('\\n') if l.strip()]\n",
        "        answer = lines[-1] if lines else \"\"\n",
        "\n",
        "    # 5Ô∏è‚É£ Show diagnostics\n",
        "    print(f\"üîé Looking for numeric token: '{truth}'\")\n",
        "    found_numbers = re.findall(r'\\d+\\.?\\d*', answer)\n",
        "    print(f\"üî¢  Extracted numbers: {found_numbers}\")\n",
        "\n",
        "    # 6Ô∏è‚É£ Check correctness\n",
        "    correct = truth in found_numbers\n",
        "    mark = '‚úÖ' if correct else '‚ùå'\n",
        "\n",
        "    # 7Ô∏è‚É£ Print the answer and result\n",
        "    print(f\"üó®Ô∏è  Answer: {answer}\")\n",
        "    print(f\"{mark}  ({elapsed:.2f}s, ŒîRAM={mem_delta}‚ÄØMiB)\")\n",
        "\n",
        "    # 8Ô∏è‚É£ Log to CSV\n",
        "    with open(output_file, 'a', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\n",
        "            question,\n",
        "            selected_model,\n",
        "            answer,\n",
        "            correct,\n",
        "            f\"{elapsed:.2f}\",\n",
        "            mem_delta\n",
        "        ])\n",
        "\n",
        "# Final memory report\n",
        "print(f\"\\n‚úîÔ∏è  Finished running {selected_model}. Final GPU RAM used: {mem_after}‚ÄØMiB\")"
      ],
      "metadata": {
        "id": "wb2ARGg3fIAk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5. üìä Summarize & Compare Results\n",
        "#@markdown This is where everything begin to be put into a conclusion for a single model. By using pandas, this cell loads the CSV file and compute per-model metrics: Accuracy (%), Average inference time (seconds), and Memory Usage (MiB).\n",
        "\n",
        "# ‚úÖ Summary Code for Single-Model Runs (New Format)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the result file from current model\n",
        "file_path = f'results_{selected_model.replace(\":\", \"_\")}.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert types as needed\n",
        "df['correct_num'] = df['correct'].astype(int)\n",
        "df['time_s'] = df['time_s'].astype(float)\n",
        "df['mem_delta_MiB'] = df['mem_delta_MiB'].astype(float)\n",
        "\n",
        "# Calculate summary metrics\n",
        "accuracy = df['correct_num'].mean() * 100\n",
        "avg_time = df['time_s'].mean()\n",
        "avg_mem = df['mem_delta_MiB'].mean()\n",
        "\n",
        "# Create a summary DataFrame for display\n",
        "summary_df = pd.DataFrame([{\n",
        "    'Model': selected_model,\n",
        "    'Accuracy (%)': round(accuracy, 1),\n",
        "    'Avg Latency (s)': round(avg_time, 2),\n",
        "    'GPU Memory Used (MiB)': round(avg_mem, 1)\n",
        "}])\n",
        "\n",
        "# üßæ Show table\n",
        "from IPython.display import display\n",
        "print(\"üìà Overall Performance Summary:\")\n",
        "display(summary_df)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t56TBh83gGog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6. üîÄ Merge All Results & Final Comparison\n",
        "#@markdown This cell will combine all CSV file named after 'result_'. Simply, it will combine all result into one single file named 'results_combined_summary.csv'.\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# 1Ô∏è‚É£ Find all result CSVs in this directory\n",
        "pattern = \"results_*.csv\"\n",
        "files = glob.glob(pattern)\n",
        "if not files:\n",
        "    raise FileNotFoundError(f\"No files matching {pattern} found!\")\n",
        "\n",
        "# 2Ô∏è‚É£ Load each file and extract the model name from its filename\n",
        "all_dfs = []\n",
        "for fn in files:\n",
        "    # e.g. fn = \"results_phi3_3.8b.csv\"\n",
        "    model_name = os.path.basename(fn).removeprefix(\"results_\").removesuffix(\".csv\")\n",
        "    df = pd.read_csv(fn)\n",
        "    df['model'] = model_name\n",
        "    all_dfs.append(df)\n",
        "\n",
        "# 3Ô∏è‚É£ Concatenate into one DataFrame\n",
        "combined = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "# 4Ô∏è‚É£ Compute per-model metrics\n",
        "combined['correct_num'] = combined['correct'].astype(int)\n",
        "combined['time_s']      = combined['time_s'].astype(float)\n",
        "combined['mem_delta_MiB'] = combined['mem_delta_MiB'].astype(float)\n",
        "\n",
        "summary = combined.groupby('model').agg(\n",
        "    Accuracy       = ('correct_num',     lambda x: x.mean() * 100),\n",
        "    Avg_Latency_s  = ('time_s',          'mean'),\n",
        "    Avg_Mem_Use_MiB= ('mem_delta_MiB',   'mean')\n",
        ").round(2).reset_index()\n",
        "\n",
        "# 5Ô∏è‚É£ Display the final comparison table\n",
        "from IPython.display import display\n",
        "print(\"üèÅ Final Comparison of All Models:\")\n",
        "display(summary)\n",
        "\n",
        "# üéâ Optional: save this summary if you like\n",
        "summary.to_csv(\"results_combined_summary.csv\", index=False)\n",
        "print(\"Saved combined summary to results_combined_summary.csv\")\n",
        "\n",
        "# üéâ End of experiment. Review the printed logs and summary to draw conclusions!"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2BqdjkzF5Biq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}