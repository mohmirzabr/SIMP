{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhX0ekpWW5JC"
      },
      "outputs": [],
      "source": [
        "# üé® Math LLM Study Assistant Experiment Notebook\n",
        "#@title üë®‚Äçüî¨ Simple Mathematical Benchmarking for Natural Language Processing Models\n",
        "#@markdown This notebook is intended for research purposes only. Based on a legacy notebook from the repository (SIMP/legacy.ipynb) which is developed together with ChatGPT by OpenAI that I attributed to. This notebook guides you step-by-step to compare atleast three natural language processing models from Ollama on basic math questions from GSK8M (openai/gsk8m).\n",
        "#@markdown <br> <br> This project is so simple that it only could benchmark simple mathemathics like GSM8K. As the result comparing only compare the numbers from the generation result and looking for the supposedly correct number, whether it is mentioned in the generation result or not.\n",
        "#@markdown <br> <br> In conclusion, this notebook isn't suitable for advanced benchmarking and only can process simple mathematical prompts.\n",
        "#@markdown <br> <br> Visit me on [GitHub](https://github.com/mohmirzabr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 0. ‚ö†Ô∏è MAKE SURE YOU'RE USING NVIDIA T4 AS THE GPU AND PYTHON 3 FOR THE RUNTIME TYPE\n",
        "#@markdown This cell will make sure whether you're having your NVIDIA T4 enabled or not.\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Q1loa5PDhGcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. üß∞ Install and Prepare Ollama\n",
        "#@markdown In this cell, we will install the backend first. In which, we'll be using Ollama. In this cell, we'll also pull those three models to be tested in the next cells.\n",
        "#@markdown <br> <br> Below are the variables of each model that you want to use. Make sure that the models are existent in Ollama to be downloaded.\n",
        "model1 = \"tinyllama:1.1b\" # @param {\"type\":\"string\"}\n",
        "model2 = \"phi3:3.8b\" # @param {\"type\":\"string\"}\n",
        "model3 = \"mistral:7b\" # @param {\"type\":\"string\"}\n",
        "\n",
        "# üöÄ Installing Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# üî• Enabling Ollama\n",
        "import subprocess\n",
        "process = subprocess.Popen (\"ollama serve\", shell=True)\n",
        "\n",
        "# ‚ôªÔ∏è Pulling the models.\n",
        "!ollama pull {model1}\n",
        "!ollama pull {model2}\n",
        "!ollama pull {model3}\n",
        "\n",
        "# üì¶ Confirming the models that are ready.\n",
        "!ollama list"
      ],
      "metadata": {
        "id": "_ScW7aixX4m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. üìÇ Load Math Questions\n",
        "#@markdown This cell is where we're going to put the samples to be tested to each model. This cell will create a file named 'math_questions.txt' with lines: question<TAB>answer (except if the file existed in the first place).\n",
        "#@markdown <br> <br> If you haven't prepare any datasets, you can use the template below.\n",
        "templatequestions = False # @param {\"type\":\"boolean\"}\n",
        "\n",
        "#@markdown <br> <br> Below are the variables that are going to be used for scraping the dataset from Hugging Face to 'math_questions.txt'.\n",
        "#@markdown <br> <br> 1. üß∞ Which dataset you're going to refer to?\n",
        "dataset = \"openai/gsm8k\" # @param {\"type\":\"string\"}\n",
        "branch = \"main\" # @param {\"type\":\"string\"}\n",
        "#@markdown <br> <br> 2. üìä How many examples to use?\n",
        "NUM_SAMPLES = 30 # @param {\"type\":\"number\"}\n",
        "\n",
        "if templatequestions == False:\n",
        "  from pathlib import Path\n",
        "\n",
        "  questions_file = Path('math_questions.txt')\n",
        "\n",
        "  if not questions_file.exists():\n",
        "      # üîß 1. Install and import the HF datasets library\n",
        "      !pip install -q datasets\n",
        "      from datasets import load_dataset\n",
        "\n",
        "      # üîç 2. Load the GSM8K training split\n",
        "      gsm8k = load_dataset(dataset, branch, split=\"train\")\n",
        "\n",
        "      # ‚úÇÔ∏è 3. Extract question + final answer (after \"####\")\n",
        "      samples = []\n",
        "      for ex in gsm8k.select(range(NUM_SAMPLES)):\n",
        "          q_text = ex[\"question\"].strip()\n",
        "          # the answer field has steps + \"#### <final>\"\n",
        "          raw_ans = ex[\"answer\"]\n",
        "          # split on \"####\" to get the last line as our truth\n",
        "          truth = raw_ans.split(\"####\")[-1].strip()\n",
        "          samples.append((q_text, truth))\n",
        "\n",
        "      # üìù 4. Save those to math_questions.txt as \"question‚êâanswer\"\n",
        "      with open(questions_file, 'w') as f:\n",
        "          for q, a in samples:\n",
        "              f.write(f\"{q}\\t{a}\\n\")\n",
        "\n",
        "      print(f\"‚úÖ Pulled {NUM_SAMPLES} samples from {dataset} and wrote '{questions_file}'\")\n",
        "\n",
        "  # üìñ 5. Now read back the file into `questions` for your experiment loop\n",
        "  questions = []\n",
        "  with open(questions_file) as f:\n",
        "      for line in f:\n",
        "          q, a = line.strip().split(\"\\t\")\n",
        "          questions.append((q, a))\n",
        "\n",
        "  print(f\"‚úÖ Loaded {len(questions)} questions for testing!\")\n",
        "elif templatequestions == True:\n",
        "  from pathlib import Path\n",
        "\n",
        "  questions_file = Path('math_questions.txt')\n",
        "  if not questions_file.exists():\n",
        "      # Create a sample file with simple math problems\n",
        "      sample = [\n",
        "          \"12 x 7\\t84\",\n",
        "          \"17 * 24\\t408\",\n",
        "          \"256 / 8\\t32\",\n",
        "          \"13 + 49\\t62\"\n",
        "      ]\n",
        "      with open(questions_file, 'w') as f:\n",
        "          for line in sample:\n",
        "              f.write(line + '\\n')  # üìù writing sample questions\n",
        "\n",
        "  # Read the questions into a list\n",
        "  questions = []\n",
        "  with open(questions_file) as f:\n",
        "      for line in f:\n",
        "          q, a = line.strip().split(\"\\t\")\n",
        "          questions.append((q, a))\n",
        "\n",
        "  print(f\"‚úÖ Loaded {len(questions)} questions for testing!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rO_2YF4va57s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üèÅ 3. Set Up Results Logging\n",
        "#@markdown We will record: question, model, answer, correct (True/False), time in seconds and put it into a CSV file to be processed by the next cell and then compared for conclusion.\n",
        "\n",
        "import csv\n",
        "\n",
        "output_file = 'results.csv'\n",
        "with open(output_file, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    # Header row for our CSV\n",
        "    writer.writerow(['question', 'model', 'answer', 'correct', 'time_s'])\n",
        "print(f\"üóÑÔ∏è  Results will be saved in: {output_file}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LlQYcfgue04P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. üöÄ Experiment is running!\n",
        "#@markdown This is the cell where everything begin to run, and the GPU RAM start to shrinking. The notebook is processing all of the questions and comparing the answer at the same time!\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "models = [model1, model2, model3]  # The Ollama models to test\n",
        "\n",
        "for question, truth in questions:\n",
        "    print(f\"\\nüîç [Q] {question}\")\n",
        "    for model in models:\n",
        "        start = time.time()\n",
        "\n",
        "        result = subprocess.run(\n",
        "            ['ollama', 'run', model, question],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True\n",
        "        )\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        if result.returncode != 0:\n",
        "            print(f\"‚ö†Ô∏è Error from {model}:\")\n",
        "            print(result.stderr)\n",
        "            continue  # Skip logging for this model if it failed\n",
        "\n",
        "        # Get the last non-empty line of output\n",
        "        lines = [line for line in result.stdout.strip().split('\\n') if line.strip()]\n",
        "        answer = lines[-1] if lines else \"\"\n",
        "\n",
        "        # Check if model's output starts with the correct number\n",
        "        # Check correctness by searching for the expected answer number in the output\n",
        "        correct = truth in answer.replace('=', '').replace('.', '').lower()\n",
        "        mark = '‚úÖ' if correct else '‚ùå'\n",
        "        print(f\"üîé Checking if '{truth}' in ‚Üí {answer}\")\n",
        "\n",
        "        print(f\"{model:<8} ‚û°Ô∏è  {answer:<30} {mark}  ({elapsed:.2f}s)\")\n",
        "\n",
        "        # Save result to CSV\n",
        "        with open(output_file, 'a', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([question, model, answer, correct, f\"{elapsed:.2f}\"])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wb2ARGg3fIAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5. üìä Summarize & Compare Results\n",
        "#@markdown This is where everything begin to be put into a conclusion. By using pandas, this cell loads the CSV file and compute per-model metrics: Accuracy (%) and Average inference time (seconds).\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(output_file)\n",
        "# Convert 'correct' to numeric for aggregation\n",
        "\n",
        "df['correct_num'] = df['correct'].astype(int)\n",
        "summary = df.groupby('model').agg(\n",
        "    accuracy = ('correct_num', 'mean'),\n",
        "    avg_time = ('time_s', lambda x: x.astype(float).mean())\n",
        ")\n",
        "# Format metrics nicely\n",
        "summary['accuracy'] = (summary['accuracy'] * 100).round(1)\n",
        "summary['avg_time'] = summary['avg_time'].round(2)\n",
        "\n",
        "print(\"\\nüìà Overall Performance Summary:\")\n",
        "print(summary)\n",
        "\n",
        "# You can also display the DataFrame directly in Colab for interactivity\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(summary)\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# üéâ End of experiment. Review the printed logs and summary to draw conclusions!"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t56TBh83gGog"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}